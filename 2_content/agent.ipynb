{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd3df8b",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56c965",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22343a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goldenmeta/Github/Career-Agent/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "import gradio as gr\n",
    "import agent_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859402fd",
   "metadata": {},
   "source": [
    "## Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c91cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Setup\n",
    "load_dotenv(override=True)\n",
    "openai = OpenAI()\n",
    "\n",
    "# Pushover Setup\n",
    "pushover_user = os.getenv(\"PUSHOVER_USER\")\n",
    "pushover_token = os.getenv(\"PUSHOVER_TOKEN\")\n",
    "pushover_url = \"https://api.pushover.net/1/messages.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f01bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function will push message in the form of a payload to the phone based on predetermined parameters. \n",
    "def push_message(message):\n",
    "    print(f\"Push: {message}\")\n",
    "    payload = {\"user\": pushover_user, \"token\": pushover_token, \"message\": message}\n",
    "    requests.post(pushover_url, data=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1a92a",
   "metadata": {},
   "source": [
    "# Tools and Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a93ed",
   "metadata": {},
   "source": [
    "## Tools\n",
    "* Tool 1: Which will handle collecting the user's information in the case they would like to contact 'Kailas' or the pdf profile user.\n",
    "* Tool 2: Which will handle collecting an questions which was or cannot be answered by the LLM. The tool will send a ping notification to my phone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233ad3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function if whether the user would like to be in touch with us\n",
    "def record_user_details(email, name=\"name not provided\", notes=\"notes not provided\"):\n",
    "    # if provided with only email was provided\n",
    "    if name == \"name not provided\" and notes == \"notes not provided\":\n",
    "        push_message(f'{email} has shown interest in connecting with you!')\n",
    "    # If the email and name was provided, but not notes. \n",
    "    elif name != \"name not provided\" and notes == \"notes not provided\":\n",
    "        push_message(f'{name} has shown interest in connecting with you!\\nEmail: {email}')\n",
    "    # if the email and notes are provided, but not the name. \n",
    "    elif name == \"name not provided\" and notes != \"notes not provided\":\n",
    "        push_message(f'{email} has shown interest in connecting with you!\\nNotes: {notes}')\n",
    "    else:\n",
    "        push_message(f\"{name} has show interest in connecting with you!\\nEmail: {email}\\nNotes: {notes}\")\n",
    "    return {\"Recorded:\": \"ok\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f534a457",
   "metadata": {},
   "source": [
    "## Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f47b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_calls(tool_calls):\n",
    "    '''\n",
    "    A function which takes in a list of tool calls, and executes them (sends a notification to phone) based on their prompt.\n",
    "    Furthermore, each tool is already pre-mapped to a function, all this function does it search and execute like a 'dumb' agent. \n",
    "    '''\n",
    "    results = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        print(f\"Tool called: {tool_name}\", flush=True)\n",
    "        tool = globals().get(tool_name)\n",
    "        result = tool(**arguments) if tool else {}\n",
    "        results.append({\"role\": \"tool\",\"content\": json.dumps(result),\"tool_call_id\": tool_call.id})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee5969",
   "metadata": {},
   "source": [
    "# Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224bc714",
   "metadata": {},
   "source": [
    "## PDF-Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2ee565",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MacOS\n",
    "'''\n",
    "pdf_path = r\"/Users/goldenmeta/Github/Career-Agent/3_linkedln/LinkedlnProfilePDF.pdf\"\n",
    "website_path = r\"/Users/goldenmeta/Github/Career-Agent/3_linkedln/LinkedlnProfileWebsite.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea39a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(pdf_path)\n",
    "linkedin = \"\"\n",
    "website = \"\"\n",
    "\n",
    "# Reads the pdf and compiles into a singular string.\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n",
    "\n",
    "\n",
    "# Reads the website link (i could paste the weblink here instead, however for sourcing purposes I will do it in a .txt file)\n",
    "with open(website_path, 'r') as link:\n",
    "    linkedln_link = link.readline()\n",
    "\n",
    "# This can be your own name (replace with with Kailas Thonnithodi with your own name)\n",
    "full_name = \"Kailas Thonnithodi\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a002aa88",
   "metadata": {},
   "source": [
    "## System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d279b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = f\"You are acting as {full_name}. You are answering questions on {full_name}'s website, \\\n",
    "    particularly questions related to {full_name}'s career, background, skills and experience. \\\n",
    "    Your responsibility is to represent {full_name} for interactions on the website as faithfully as possible. \\\n",
    "    Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "    The following are tools which you can called in certain scenarios. \\\n",
    "    If the user is engaging in discussion, try to steer them towards getting in touch via email; ask for their email and record it using \\'record_user_details\\' tool. \\\n",
    "    Also I would like to emphasis that I want you to call only one tool at a time (if called, basically each time the user finishes their prompt).\"\n",
    "\n",
    "sys_prompt += f\"\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "sys_prompt += f\"\\n\\n## LinkedIn Link:\\n{website}\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d0e5b9",
   "metadata": {},
   "source": [
    "## Chat function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ba038",
   "metadata": {},
   "source": [
    "## Evaluator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e272e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff68652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving the evaluator an initalisatino state; the purpose of the evaluating another model's response. \n",
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {full_name} and is representing {full_name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {full_name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback. Furthermore give it a score from 1-10.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d08000c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving the evaluator tool a user prompt. This will check how well the agent was able to give a response based on action. \n",
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Heres's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += f\"Please evaluate the response, replying with whether it is acceptable.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfdfb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_model = OpenAI(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta\"\n",
    ")\n",
    "\n",
    "evaluator_model_name = \"gemini-2.0-flash\"\n",
    "agent_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f51f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an evaluation of the model's response to the orignal user's sentence. \n",
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "    messages = [{\"role\": \"system\",\"content\": evaluator_system_prompt}] + [{\"role\":\"user\",\"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = evaluator_model.chat.completions.parse(model=evaluator_model_name, messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "603482ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a rerunning features which will create a new instance of the model which will then be used in asking the question again. \n",
    "# furthermore, the question which will be targetted will based on the score; if the score is below 1-5, this will lead to a less purpose solution.\n",
    "# if the score is more than 5, this is considered acceptable, therefore continue, else re run, thinking and come up with a better solution for the user's question.\n",
    "def rerun(reply, message, history, feedback):\n",
    "    updated_sys_prompt = sys_prompt + \"\\n\\n## Previous answer rejected.\\nYou just tried reply, however the quality control rejected your answer.\"\n",
    "    updated_sys_prompt += f\"## Your attempted answer:\\n{reply}\\n\"\n",
    "    updated_sys_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_sys_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=agent_model, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a61539",
   "metadata": {},
   "source": [
    "## Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "228f3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the following agent, I will be using the gpt 4o mini model due to it's relative light weight and efficincy when producing a reasonable answer. \n",
    "\n",
    "tools = agent_tools.tools\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": sys_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    done = False\n",
    "    while not done:\n",
    "        # the response completed during the llms call\n",
    "        response = openai.chat.completions.create(model=agent_model, messages=messages, tools=tools)\n",
    "        reply = response.choices[0].message.content\n",
    "\n",
    "        # Checking the evaluation based on the openai message board.\n",
    "        evaluation = evaluate(reply, message, history)\n",
    "        feedback = evaluation.feedback\n",
    "\n",
    "        # If the message reasoning is not acceptable (which was parsed), then rerun the model to produce an acceptable response. \n",
    "        if evaluation.is_acceptable:\n",
    "            # This is just for developers to see; making sure that the repsonse was actually gone through the evaluation too. \n",
    "            print(f\"Pass evaluation - {feedback}\")\n",
    "        else:\n",
    "            print(f\"Failed evaluation - {feedback}\")\n",
    "            reply = rerun(reply, message, history, feedback)\n",
    "\n",
    "        finish_reason = response.choices[0].finish_reason\n",
    "        # If a tool is call, use the tool handler for committing.\n",
    "        if finish_reason==\"tool_calls\":\n",
    "            message = response.choices[0].message\n",
    "            tool_calls = message.tool_calls\n",
    "            results = handle_tool_calls(tool_calls)\n",
    "            messages.append(message)\n",
    "            messages.extend(results)\n",
    "        else:\n",
    "            done = True\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e42d0",
   "metadata": {},
   "source": [
    "## Tester using Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d363cb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass evaluation - This is a great and professional response. It directly answers the user's question and offers a way to connect, which aligns with the goal of engaging with potential clients or employers.\n",
      "Pass evaluation - The Agent did not respond, which is acceptable in this case. The user has provided their email address. The agent should use this to follow up.\n",
      "Tool called: record_user_details\n",
      "Push: kaiasl@gmail.com has shown interest in connecting with you!\n",
      "Pass evaluation - The agent's response is appropriate and professional. It confirms receipt of the user's email and offers further assistance. The tone is engaging and helpful.\n",
      "Pass evaluation - The agent gracefully dodged the question while maintaining a professional tone. The response is engaging and attempts to redirect the conversation back to a professional context, which is appropriate given the scenario. The agent also includes a question back to the user, furthering the conversation.\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbe303",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "career-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
